{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, default_collate\n",
    "from timm.models.layers import LayerNorm2d\n",
    "import torchshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, AutoConfig\n",
    "from transformers import AutoImageProcessor, XLMRobertaTokenizer\n",
    "from torchscale.architecture.config import EncoderConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HpConfig:\n",
    "    img_size = 640\n",
    "    drop_path = 0.1\n",
    "    val_batch_size = 1\n",
    "    lr = 1e-4\n",
    "    weight_decay = 0.05\n",
    "    grad_ckpt = False\n",
    "\n",
    "    batch_size = 2\n",
    "    grad_acc_steps = 4\n",
    "    num_gpu = 2\n",
    "    mixed_precision='bf16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model_and_may_interpolate\n",
    "from modeling_utils import _get_large_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beit3_seg import BEiT3SegForUniversalSegmentation\n",
    "    \n",
    "mask2former_config = AutoConfig.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\", )\n",
    "mask2former_config.backbone_config = dict(\n",
    "    beit3_args=_get_large_config(\n",
    "        img_size=HpConfig.img_size,\n",
    "        drop_path_rate=HpConfig.drop_path,\n",
    "        checkpoint_activations=False,\n",
    "    ),\n",
    "    deform_num_heads=16,\n",
    "    deform_ratio=0.5,\n",
    "    interaction_indexes=[[0, 5], [6, 11], [12, 17], [18, 23]],\n",
    "\n",
    "    init_values=1e-6,\n",
    "    conv_inplane=64,\n",
    "    n_points=4,\n",
    "    cffn_ratio=0.25,\n",
    "    with_cp=HpConfig.grad_ckpt,\n",
    "    num_segments = 1000,\n",
    ")\n",
    "mask2former_config.backbone_dim = 1024\n",
    "mask2former_config.num_labels = 3\n",
    "\n",
    "mask2former_config.use_text_cross_attn = True\n",
    "mask2former_config.use_text_features = True\n",
    "mask2former_config.use_text_contrastive_loss = True\n",
    "mask2former_config.use_objectness_loss = False\n",
    "\n",
    "mask2former_config.match_once_only = False\n",
    "mask2former_config.drop_first_ce_loss = False\n",
    "mask2former_config.encoder_layers=6\n",
    "mask2former_config.decoder_layers=10\n",
    "\n",
    "beit3_seg = BEiT3SegForUniversalSegmentation(mask2former_config)\n",
    "beit3_seg = beit3_seg.apply(beit3_seg._init_weights)\n",
    "beit3_seg.model.pixel_level_module.encoder.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beit3_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer(\"./beit3.spm\")\n",
    "tokenizer.add_tokens([\"<WLS>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"<WLS>dog;<WLS>cat;<WLS>rabbit;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "pixel_values = torch.randn(bs, 3, HpConfig.img_size, HpConfig.img_size)\n",
    "input_ids = tokenizer([\"<WLS>dog;<WLS>cat;<WLS>rabbit;\"]*bs, return_tensors=\"pt\")[\"input_ids\"]\n",
    "cat_input_ids = torch.tensor([[0, 3, 6] for _ in range(bs)])\n",
    "mask_labels =[torch.randint(0, 2, (2, HpConfig.img_size, HpConfig.img_size)).float().to(\"cuda\") for _ in range(bs)]\n",
    "class_labels = torch.tensor([[1,2] for _ in range(bs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "beit3_seg = beit3_seg.to(\"cuda\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = beit3_seg(\n",
    "        pixel_values=pixel_values.to(\"cuda\"),\n",
    "        input_ids=input_ids.to(\"cuda\"),\n",
    "        cat_input_ids=cat_input_ids.to(\"cuda\"),\n",
    "        mask_labels=mask_labels,\n",
    "        class_labels=class_labels.to(\"cuda\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
