{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "from torch.utils.data import Dataset, DataLoader, default_collate\n",
    "from timm.models.layers import LayerNorm2d\n",
    "import torchshow\n",
    "\n",
    "from utils import load_model_and_may_interpolate\n",
    "from modeling_utils import _get_base_config, _get_large_config\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_scale.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, AutoConfig\n",
    "from transformers import AutoImageProcessor, XLMRobertaTokenizer\n",
    "from torchscale.architecture.config import EncoderConfig\n",
    "from lion_pytorch import Lion\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from accelerate import notebook_launcher, DistributedDataParallelKwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BEiT3_adapter.panoptic_dataset import COCOPanopticDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HpConfig:\n",
    "    img_size = 640\n",
    "    drop_path = 0.1\n",
    "    batch_size = 2\n",
    "    val_batch_size = 1\n",
    "    grad_acc_steps = 1\n",
    "    lr = 1e-4\n",
    "    weight_decay = 0.05\n",
    "    grad_ckpt = False\n",
    "    num_gpu = 2\n",
    "    mixed_precision='bf16'\n",
    "    wls_token = '<wls>'\n",
    "    sep_token = '‚ñÅ;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(accelerator):\n",
    "    tokenizer = XLMRobertaTokenizer(\"../beit3_weights/beit3.spm\")\n",
    "    tokenizer.add_tokens([HpConfig.wls_token, HpConfig.sep_token])\n",
    "    \n",
    "    coco_mask2former_processor = AutoImageProcessor.from_pretrained(\n",
    "        \"facebook/mask2former-swin-base-coco-panoptic\",\n",
    "        do_resize=False, do_rescale=True, do_normalize=True, ignore_index=0,\n",
    "    )\n",
    "    ade_mask2former_processor = AutoImageProcessor.from_pretrained(\n",
    "        \"facebook/mask2former-swin-large-ade-panoptic\",\n",
    "        do_resize=False, do_rescale=True, do_normalize=True, ignore_index=0,\n",
    "    )\n",
    "\n",
    "    train_transform = A.Compose(\n",
    "        [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.SmallestMaxSize([HpConfig.img_size*i//10 for i in range(5, 21)], p=1.0),\n",
    "            A.PadIfNeeded(\n",
    "                HpConfig.img_size, HpConfig.img_size,\n",
    "                position=A.PadIfNeeded.PositionType.TOP_LEFT,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "                ),\n",
    "            A.RandomCrop(HpConfig.img_size, HpConfig.img_size),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    val_trainform = A.Compose(\n",
    "        [\n",
    "            A.LongestMaxSize(HpConfig.img_size, p=1.0),\n",
    "            A.PadIfNeeded(\n",
    "                HpConfig.img_size, HpConfig.img_size,\n",
    "                position=A.PadIfNeeded.PositionType.TOP_LEFT,\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    with open('../../datasets/COCO/annotations/panoptic_train2017.json') as file:\n",
    "        coco_train_ann = json.load(file)\n",
    "    with open('../../datasets/COCO/annotations/panoptic_val2017.json') as file:\n",
    "        coco_val_ann = json.load(file)\n",
    "    with open('../../datasets/ADE20K/from_mmdet/ADEChallengeData2016/ade20k_panoptic_val.json') as file:\n",
    "        ade_val_ann = json.load(file)\n",
    "\n",
    "    coco_train_dataset = COCOPanopticDataset(\n",
    "        coco_train_ann,\n",
    "        '../../datasets/COCO/train2017',\n",
    "        '../../datasets/COCO/annotations/panoptic_train2017',\n",
    "        transform=train_transform,\n",
    "        processor=coco_mask2former_processor,\n",
    "        use_text=True,\n",
    "        tokenizer=tokenizer,\n",
    "        sep_token=HpConfig.sep_token,\n",
    "        use_sep=True,\n",
    "        num_sampled_label=133,\n",
    "        wls_token=HpConfig.wls_token,\n",
    "        max_sep_num=3,\n",
    "        # use_sep=False,\n",
    "    )\n",
    "\n",
    "    coco_val_dataset = COCOPanopticDataset(\n",
    "        coco_val_ann,\n",
    "        '../../datasets/COCO/val2017',\n",
    "        '../../datasets/COCO/annotations/panoptic_val2017',\n",
    "        transform=val_trainform,\n",
    "        processor=coco_mask2former_processor,\n",
    "        use_text=True,\n",
    "        tokenizer=tokenizer,\n",
    "        sep_token=HpConfig.sep_token,\n",
    "        use_sep=True,\n",
    "        num_sampled_label=133,\n",
    "        wls_token=HpConfig.wls_token,\n",
    "        max_sep_num=1,\n",
    "        # use_sep=False,\n",
    "    )\n",
    "\n",
    "    ade_val_dataset = COCOPanopticDataset(\n",
    "        ade_val_ann,\n",
    "        '../../datasets/ADE20K/from_mmdet/ADEChallengeData2016/images/validation',\n",
    "        '../../datasets/ADE20K/from_mmdet/ADEChallengeData2016/ade20k_panoptic_val',\n",
    "        transform=val_trainform,\n",
    "        processor=ade_mask2former_processor,\n",
    "        use_text=True,\n",
    "        tokenizer=tokenizer,\n",
    "        sep_token=HpConfig.sep_token,\n",
    "        use_sep=True,\n",
    "        num_sampled_label=150,\n",
    "        wls_token=HpConfig.wls_token,\n",
    "        max_sep_num=1,\n",
    "        # use_sep=False,\n",
    "    )\n",
    "\n",
    "    def custom_collate(batch):\n",
    "        collated_batch = {}\n",
    "        \n",
    "        first_elem = batch[0]\n",
    "        if 'mask_labels' in first_elem:\n",
    "            collated_batch['mask_labels'] = [b.pop('mask_labels') for b in batch]\n",
    "        if 'class_labels' in first_elem:\n",
    "            collated_batch['class_labels'] = [b.pop('class_labels') for b in batch]\n",
    "        if 'origin_class_labels' in first_elem:\n",
    "            collated_batch['origin_class_labels'] = [b.pop('origin_class_labels') for b in batch]\n",
    "        if 'input_ids' in first_elem:\n",
    "            collated_batch.update(tokenizer.pad(\n",
    "                [{'input_ids': b.pop('input_ids')} for b in batch],\n",
    "                max_length=640, padding=True,\n",
    "            ))\n",
    "        \n",
    "        collated_batch.update(default_collate(batch))\n",
    "        \n",
    "        return collated_batch\n",
    "\n",
    "    coco_train_loader = DataLoader(\n",
    "        coco_train_dataset,\n",
    "        batch_size=HpConfig.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        collate_fn=custom_collate,\n",
    "    )\n",
    "\n",
    "    coco_val_loader = DataLoader(\n",
    "        coco_val_dataset,\n",
    "        batch_size=HpConfig.val_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        collate_fn=custom_collate,\n",
    "    )\n",
    "\n",
    "    ade_val_loader = DataLoader(\n",
    "        ade_val_dataset,\n",
    "        batch_size=HpConfig.val_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        collate_fn=custom_collate,\n",
    "    )\n",
    "\n",
    "    return coco_train_loader, coco_val_loader, ade_val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbedding(nn.Module):\n",
    "    def __init__(self, old_embedding, new_embedding, split_idx):\n",
    "        super().__init__()\n",
    "        self.old_embedding = old_embedding\n",
    "        self.new_embedding = new_embedding\n",
    "        self.split_idx = split_idx\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        old_embeds = self.old_embedding(\n",
    "            input_ids.clamp(max=self.old_embedding.num_embeddings - 1))\n",
    "        new_embeds = self.new_embedding(\n",
    "            (input_ids - self.split_idx).clamp(min=0))\n",
    "\n",
    "        return torch.where(\n",
    "            input_ids.unsqueeze(-1) < self.split_idx, old_embeds, new_embeds)\n",
    "\n",
    "def create_model(accelerator, load_weight=True, freeze_backbone=True, interpolate_pos=False, add_new_embedding=False):\n",
    "    from BEiT3_adapter.beit3_seg_ov_v2 import BEiT3SegForUniversalSegmentation\n",
    "    \n",
    "    mask2former_config = AutoConfig.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\", )\n",
    "    mask2former_config.backbone_config = dict(\n",
    "        beit3_args=_get_large_config(\n",
    "            img_size=HpConfig.img_size,\n",
    "            drop_path_rate=HpConfig.drop_path,\n",
    "            checkpoint_activations=False,\n",
    "        ),\n",
    "        deform_num_heads=16,\n",
    "        deform_ratio=0.5,\n",
    "        interaction_indexes=[[0, 5], [6, 11], [12, 17], [18, 23]],\n",
    "\n",
    "        init_values=1e-6,\n",
    "        conv_inplane=64,\n",
    "        n_points=4,\n",
    "        cffn_ratio=0.25,\n",
    "        with_cp=HpConfig.grad_ckpt,\n",
    "    )\n",
    "    mask2former_config.backbone_dim = 768\n",
    "    mask2former_config.num_labels = 133\n",
    "    mask2former_config.use_text_cross_attn = True\n",
    "    mask2former_config.use_text_features = True\n",
    "    mask2former_config.use_text_contrastive_loss = True\n",
    "    mask2former_config.use_objectness_loss = True\n",
    "    mask2former_config.match_once_only = False\n",
    "    mask2former_config.drop_first_ce_loss = True\n",
    "    mask2former_config.encoder_layers=6\n",
    "    mask2former_config.decoder_layers=10\n",
    "    mask2former_config.objectness_weight = 2\n",
    "\n",
    "    beit3_seg = BEiT3SegForUniversalSegmentation(mask2former_config)\n",
    "    beit3_seg = beit3_seg.apply(beit3_seg._init_weights)\n",
    "    beit3_seg.model.pixel_level_module.encoder.init_weights()\n",
    "\n",
    "    if load_weight:\n",
    "        if accelerator.is_main_process:\n",
    "            print('Loading BEiT3 pretraind weight...')\n",
    "            load_model_and_may_interpolate(\n",
    "                '../beit3_weights/beit3_base_patch16_224.pth',\n",
    "                beit3_seg.model.pixel_level_module.encoder,\n",
    "                'model|module',\n",
    "                'beit3.',\n",
    "            )\n",
    "            print()\n",
    "            mask2former_pretrained_weigths = torch.load('./training_checkpoints/vit_adapter_mask2former_coco_768.pth')\n",
    "            beit3_seg_param_shapes = {n:v.shape for n, v in beit3_seg.state_dict().items()}\n",
    "            for name, v_shape in [(n, v.shape) for n, v in mask2former_pretrained_weigths.items()]:\n",
    "                if name in beit3_seg_param_shapes and v_shape != beit3_seg_param_shapes[name]:\n",
    "                    print('mismatch:', name, v_shape, beit3_seg_param_shapes[name])\n",
    "                    del mask2former_pretrained_weigths[name]\n",
    "            r = beit3_seg.load_state_dict(mask2former_pretrained_weigths, strict=False)\n",
    "            print(r)\n",
    "\n",
    "    if interpolate_pos:\n",
    "        if accelerator.is_main_process:\n",
    "            with torch.no_grad():\n",
    "                origin_pos = beit3_seg.model.pixel_level_module.encoder.encoder.embed_positions.B.weight[2:130].clone()\n",
    "                new_pos = F.interpolate(origin_pos.unsqueeze(0).permute(0, 2, 1), 640, mode='linear').permute(0, 2, 1)[0]\n",
    "                beit3_seg.model.pixel_level_module.encoder.encoder.embed_positions.B.weight[2:640+2] = new_pos\n",
    "                # beit3_seg.model.transformer_module.psuedo_class_embedder[:512] = new_pos\n",
    "\n",
    "    if add_new_embedding:\n",
    "        print('Creating new embedding...')\n",
    "        old_embedding = beit3_seg.model.pixel_level_module.encoder.text_embed\n",
    "        new_embedding_init_weight = old_embedding.weight[[0]].detach().clone()\n",
    "        new_embedding = nn.Embedding(\n",
    "            1,\n",
    "            768,\n",
    "            _weight=new_embedding_init_weight,\n",
    "        )\n",
    "        beit3_seg.model.pixel_level_module.encoder.text_embed = CustomEmbedding(\n",
    "            old_embedding,\n",
    "            new_embedding,\n",
    "            64002,\n",
    "        )\n",
    "\n",
    "    if freeze_backbone:\n",
    "        freeze_keywords = [\n",
    "            'model.pixel_level_module.encoder.text_embed', \n",
    "            'model.pixel_level_module.encoder.vision_embed', \n",
    "            'model.pixel_level_module.encoder.encoder', \n",
    "        ]\n",
    "        for name, param in beit3_seg.named_parameters():\n",
    "            if any([kw in name for kw in freeze_keywords]):\n",
    "                param.requires_grad_(False)\n",
    "            else:\n",
    "                param.requires_grad_(True)\n",
    "        beit3_seg.model.pixel_level_module.encoder.encoder.embed_positions.A.requires_grad_(True)\n",
    "        beit3_seg.model.pixel_level_module.encoder.encoder.embed_positions.B.requires_grad_(True)\n",
    "        if add_new_embedding:\n",
    "            beit3_seg.model.pixel_level_module.encoder.text_embed.new_embedding.weight.requires_grad_(True)\n",
    "                \n",
    "        train_names = []\n",
    "        freeze_names = []\n",
    "        for name, param in beit3_seg.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                train_names.append(name)\n",
    "            else:\n",
    "                freeze_names.append(name)\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            for name in train_names:\n",
    "                print('o', name)\n",
    "            for name in freeze_names:\n",
    "                print('x', name)\n",
    "\n",
    "    return beit3_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(accelerator, model):\n",
    "    def get_parameter_names(model, forbidden_layer_types):\n",
    "        \"\"\"\n",
    "        Returns the names of the model parameters that are not inside a forbidden layer.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for name, child in model.named_children():\n",
    "            result += [\n",
    "                f\"{name}.{n}\"\n",
    "                for n in get_parameter_names(child, forbidden_layer_types)\n",
    "                if not isinstance(child, tuple(forbidden_layer_types))\n",
    "            ]\n",
    "        # Add model specific parameters (defined with nn.Parameter) since they are not in any child.\n",
    "        result += list(model._parameters.keys())\n",
    "        return result\n",
    "    \n",
    "    lion_lr = HpConfig.lr / 5\n",
    "    lion_weight_decay = HpConfig.weight_decay * 5\n",
    "\n",
    "    no_decay_names = [\"bias\", \"embed_positions\", \"queries_embedder\", \"psuedo_class_embedder\", \"position_embedding\"]\n",
    "    decay_parameters = get_parameter_names(model, [nn.LayerNorm, LayerNorm2d])\n",
    "    decay_parameters = [name for name in decay_parameters if all([not ndn in name for ndn in no_decay_names])]\n",
    "\n",
    "    param_groups = {\n",
    "        \"backbone_decay\": [],\n",
    "        \"backbone_no_decay\": [],\n",
    "        \"head_decay\": [],\n",
    "        \"head_no_decay\": [],\n",
    "    }\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if n in decay_parameters and 'beit3' in n:\n",
    "            param_groups['backbone_decay'].append((n, p))\n",
    "        elif not n in decay_parameters and 'beit3' in n:\n",
    "            param_groups['backbone_no_decay'].append((n, p))\n",
    "        elif n in decay_parameters and not 'beit3' in n:\n",
    "            param_groups['head_decay'].append((n, p))\n",
    "        elif not n in decay_parameters and not 'beit3' in n:\n",
    "            param_groups['head_no_decay'].append((n, p))\n",
    "        else:\n",
    "            print(f'Strange param: {n}')\n",
    "\n",
    "    # for group_name, group in param_groups.items():\n",
    "    #     print(group_name, len(group))\n",
    "    #     for n, _ in group:\n",
    "    #         print(f'    - {n}')\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in param_groups['head_decay']],\n",
    "            \"weight_decay\": lion_weight_decay,\n",
    "            \"lr\": lion_lr,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in param_groups['head_no_decay']],\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"lr\": lion_lr,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in param_groups['backbone_decay']],\n",
    "            \"weight_decay\": lion_weight_decay,\n",
    "            \"lr\": lion_lr*0.2,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in param_groups['backbone_no_decay']],\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"lr\": lion_lr*0.2,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = Lion(\n",
    "        optimizer_grouped_parameters,\n",
    "        # lr=lion_lr,\n",
    "        # weight_decay=lion_weight_decay,\n",
    "    )\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < 2000*HpConfig.num_gpu:\n",
    "            return step/(2000*HpConfig.num_gpu)\n",
    "        elif step > 40000*HpConfig.num_gpu:\n",
    "            return 0.01\n",
    "        elif step > 30000*HpConfig.num_gpu:\n",
    "            return 0.1\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    return optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(seed: int = 42):\n",
    "    set_seed(seed)\n",
    "    # Initialize accelerator\n",
    "    kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=HpConfig.mixed_precision,\n",
    "        gradient_accumulation_steps=HpConfig.grad_acc_steps,\n",
    "        log_with='wandb',\n",
    "        kwargs_handlers=[kwargs],\n",
    "    )\n",
    "\n",
    "    # Build dataloaders\n",
    "    coco_train_loader, coco_val_loader, ade_val_loader = get_dataloaders(accelerator)\n",
    "    # model = create_model(accelerator, load_weight=True, freeze_backbone=True, interpolate_pos=True)\n",
    "    model = create_model(\n",
    "        accelerator, load_weight=False, freeze_backbone=True,\n",
    "        interpolate_pos=False, add_new_embedding=True,\n",
    "        )\n",
    "\n",
    "    # if accelerator.is_local_main_process:\n",
    "    #     model.model.transformer_module = torch.compile(\n",
    "    #         model.model.transformer_module,\n",
    "    #         # mode='max-autotune',\n",
    "    #     )\n",
    "\n",
    "    optimizer, lr_scheduler = configure_optimizer(accelerator, model)\n",
    "\n",
    "    coco_train_loader, coco_val_loader, ade_val_loader = accelerator.prepare(\n",
    "        coco_train_loader, coco_val_loader, ade_val_loader\n",
    "    )\n",
    "    model, optimizer, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, lr_scheduler\n",
    "    )\n",
    "    # lr_scheduler.step_with_optimizer = False\n",
    "\n",
    "    model.module.model.pixel_level_module.encoder.encoder = torch.compile(\n",
    "        model.module.model.pixel_level_module.encoder.encoder,\n",
    "        mode='max-autotune',\n",
    "        # mode=\"reduce-overhead\",\n",
    "    )\n",
    "    # model.module.model.transformer_module = torch.compile(\n",
    "    #     model.module.model.transformer_module,\n",
    "    #     mode='max-autotune',\n",
    "    #     # mode=\"reduce-overhead\",\n",
    "    # )\n",
    "\n",
    "    # if accelerator.is_local_main_process:\n",
    "    #     accelerator.init_trackers(\n",
    "    #         \"BEiT3_Seg_Acc\",\n",
    "    #         config={\n",
    "    #             'img_size': 640,\n",
    "    #         },\n",
    "    #     )\n",
    "    #     wandb.run.log_code(\n",
    "    #         \".\",\n",
    "    #         include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"),\n",
    "    #     )\n",
    "\n",
    "    global_step = 0\n",
    "    while global_step < 50000:\n",
    "        model.train()\n",
    "        for loader_step, batch in enumerate(tqdm(coco_train_loader, disable=not accelerator.is_local_main_process)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(\n",
    "                pixel_values=batch['pixel_values'],\n",
    "                input_ids=batch['input_ids'],\n",
    "                cat_input_ids=batch['cat_token_idxs'],\n",
    "                text_padding_position=1-batch['attention_mask'],\n",
    "                class_labels=batch['origin_class_labels'],\n",
    "                mask_labels=batch['mask_labels'],\n",
    "                return_loss_dict=True,\n",
    "            )\n",
    "\n",
    "            accelerator.backward(outputs.loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            if not accelerator.optimizer_step_was_skipped:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            step_log = {\n",
    "                \"train\": {'loss': outputs.loss},\n",
    "                \"losses\": outputs.loss_dict,\n",
    "                \"learning rates\": {f\"group_{i}\":lr for i, lr in enumerate(lr_scheduler.get_last_lr())},\n",
    "            }\n",
    "            accelerator.log(step_log, step=global_step)\n",
    "\n",
    "            if global_step != 0 and global_step % 4000 == 0 or global_step == 2000:\n",
    "                accelerator.print(f'Saving model on step: {global_step}..')\n",
    "                accelerator.wait_for_everyone()\n",
    "                accelerator.save_state(f'training_checkpoints/adapter-v11-{global_step}')\n",
    "                # accelerator.save_model(model, f'training_checkpoints/adapter-v2-{global_step}')\n",
    "                accelerator.print('Model Saved!')\n",
    "\n",
    "            # accelerator.print(f'Saveing training state on step: {global_step}..')\n",
    "            # accelerator.save_state(output_dir=\"latest-training-state\")\n",
    "            # accelerator.print('State Saved!')\n",
    "\n",
    "            if global_step >= 90000:\n",
    "                break\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = (96, )\n",
    "# notebook_launcher(training_loop, args, num_processes=HpConfig.num_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
